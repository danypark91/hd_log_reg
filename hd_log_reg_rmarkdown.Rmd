---
title: "Heart Disease UCI - Logistic Regression"
author: "Dany Park"
date: "11/02/2021"
output: rmarkdown::github_document
---

# Logistic Regression and Prediction of Heart Disease
The project is to apply logistic regression to the heart disease datatset and apply the fitted model to predict the potential patient.

## 1. Overview of Logistic Regression
Consider a dataset with the response variable(Y) falls into two categories. Logistic regression is a statistical model that focuses on the probability of each Y variable's class given values of the independent variables(X). Since the model is based on the probability, at any given X value, the result has to fall between 0 and 1. Mathematically speaking, 0 ≤ P(X) ≤ 1. The value of P(X) should always produce the outputs within the range. 

![Logistic Regression](https://www.dotnetlovers.com/Images/LogisticRegressionFormula1020201890212AM.png)

The regression is fitted using the maximum likelihood method because of its statistical properties. The method estimates the model's coefficients such that the predicted probability corresponds as closely as possible to the individual's observations. In other words, predicted β0 and β1 are found such that plugging these estimates into p(X), yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not¹.

## 2. The Dataset 
The original [dataset](https://archive.ics.uci.edu/ml/datasets/Heart+Disease) from UCI contained 76 attributes which represent a patient's condition. The dataset for this article is from [Kaggle - Heart Disease UCI](https://www.kaggle.com/ronitf/heart-disease-uci). The subset of 14 attributes with every incident represents a patient.

| Attribute | Description |
| :---: | :---: |
| target | Patients with heart disease, Response variable |
| age | Age of patients |
| sex | Gender of patients |
| cp | chest pain type (4 values) |
| trestbps | resting blood pressure |
| sr | serum cholestoral in mg/dl |
| fbs | fasting blood sugar > 120 mg/dl |
| restang | resting electrocardiographic results (values 0,1,2) |
| hr | maximum heart rate achieved |
| exang | exercise induced angina |
| oldpeak | ST depression induced by exercise relative to rest |
| slope | the slope of the peak exercise ST segment |
| fbs | number of major vessels (0-3) colored by flourosopy |
| thal | 3 = normal; 6 = fixed defect; 7 = reversable defect |

The data is imported from the local device and printed summary of the dataframe to get the overview. 
```{r Importation, echo=TRUE}
#import dataframe: csv
df<-read.csv("heart.csv", header =TRUE)
summary(df)
head(df,5)
```
The dataset has total of 14 attributes and 303 incidences. Of those, `sex`, `cp`, `fbs`, `restecg`, `exang`, `slope`, `thal` and `target` are the characterized as categorical variable. The rest, `age`, `trestbps`, `chol`, `thalach` and `oldpeak`, are the continuous variable. However, before we proceed, there's an erronous attribute: `ï..age`. It is corrected to `age`. 
```{r Attribute name change, echo=TRUE}
#change erronous attribute name:  ï..age
colnames(df)[colnames(df)=='ï..age'] <-'age'
head(df,5)
```
Most of the time, the categorical variable in the dataframe requires to be coverted to the factor type in Rstudio. Initially, all the variables were categorized as integer. Before analyze the data, categorical variables must be converted from int to factor. The code `as.factor` is used to successfully convert. As shown below, the result between the first `str` and the second run are different. In fact, the second `str` overrided the type accordingly.
```{r Change of attribute type, echo=TRUE}
#check the type of each attribute and change to factor or int
str(df)

df$sex <- as.factor(df$sex)
df$cp <- as.factor(df$cp)
df$fbs <- as.factor(df$fbs)
df$restecg <- as.factor(df$restecg)
df$exang <- as.factor(df$exang)
df$slope <- as.factor(df$slope)
df$thal <- as.factor(df$thal)

str(df) #re-run
```
One of the most important step in data analysis is dealing with the missing data. Usually, the complete dataset doesn't take place in the industry level. The code `missmap` is the perfect tool to visualize the missing data in each attribute. Luckily, the graph suggests that there isn't any missing data for this dataframe. 
```{r Missmap, echo=TRUE}
#check for any missing data
library(Amelia)
missmap(df, main = "Missing Data vs Observed")
```

The final step before analyze the data is checking the ratio of the categories in the response variable. Resampling method can resolve the high inbalance between the classification. However, below result provides that the imbalance isn't significant enough to apply under/over-sampling algorithm.
```{r Count each categorical value, echo=TRUE}
#check the ratio of reponse variable and see if it requires to rebalance
library(plyr)
count(df,vars="target")
```

## 3. Data Visualization
Before 
```{r Data Visualization Package,echo=TRUE}
#Data Visualization
library(GGally)
library(ggsci)
```

```{r Frequency of Target, echo=TRUE, fig.height=5, fig.width=4}
#1. Count of Target variable
ggplot(df, aes(factor(target), fill=factor(target)))+
  geom_bar(stat="count", width=0.5, color="black")+
  ggtitle("Count of Target")+xlab("Target")+ylab("Count")+labs(fill="Target")+
  theme_bw()+
  scale_fill_npg()
```

```{r Frequency of Gender broken by Target, echo=TRUE, fig.height=5, fig.width=6}
#2. Barplot of Gender broken down by Target
ggplot(df, aes(sex, fill=factor(target)))+
  geom_bar(stat="count", width=0.5, color="black", position=position_dodge())+
  ggtitle("Gender of Patient, Broken by Target")+xlab("Gender")+ylab("Count")+labs(fill="Target")+
  theme_bw()+
  scale_fill_npg()
```

```{r For Loop Barplot of Categorical Variables, echo=TRUE, fig.height=5, fig.width=6}
#3. Barplot for categorical variables broken down by Target
cat_var <- c("sex", "cp", "fbs", "restecg", "exang", "slope", "thal", "target")
cat_df <- df[cat_var]

for(i in 1:7){
  print(ggplot(cat_df, aes(x=cat_df[,i], fill=factor(target)))+
    geom_bar(stat="count", width=0.5, color="black", position=position_dodge())+
    ggtitle(paste(colnames(cat_df)[i], ": Broken by Target"))+
    xlab(colnames(cat_df)[i])+ylab("Count")+labs(fill="Target")+
    theme_bw()+
    scale_fill_npg())
}
```

```{r Boxplot of Age broken by Target, echo=TRUE}
library(ggpubr)
#4. Age distribution by Target
mp <- ggplot(df, aes(sex, age, fill=factor(target)))+
  geom_boxplot(width=0.5)+
  ggtitle("Distribution of Age, by Gender", subtitle="")+xlab("Gender")+ylab("Age")+labs(fill="Target")+
  theme_bw()+
  scale_fill_npg()
#4-1. subplots
xplot <- ggplot(df, aes(sex, fill=factor(target)))+
  geom_bar(stat="count", width=0.5, alpha=0.4,color="black", position=position_dodge())+
  labs(fill="Target")+
  theme_bw()+
  scale_fill_npg()
yplot <- ggplot(df, aes(age, fill=factor(target)))+
  geom_density(alpha=0.4)+
  labs(fill="Target")+
  theme_bw()+
  scale_fill_npg()+
  rotate()
#4-2. combination
library(ggpubr)
ggarrange(xplot, NULL, mp, yplot,
          ncol =2, nrow=2, align = "hv",
          widths= c(3,1), heights = c(1,2),
          common.legend= TRUE)
```

```{r For Loop Boxplot of Continuous Variables, echo=TRUE}
#5. Boxplot for continuous variables broken down by Target
cont_var <- c("age", "trestbps", "chol", "thalach", "oldpeak", "target")
cont_df <- df[cont_var]

for(i in 1:5){
  print(ggplot(cont_df, aes(x=cont_df[,i], y=factor(target), fill=factor(target)))+
          geom_boxplot(width=0.5)+
          geom_point(position=position_dodge(width=0.5), alpha=0.2)+
          ggtitle(paste(colnames(cont_df)[i], ": Broken by Target"))+
          xlab(colnames(cont_df)[i])+ylab("Target")+labs(fill="Target")+
          theme_bw()+
          scale_fill_npg())
}
```

## 4. Logistic Regression
In the beginning of this notebook, the concept of the logistic regression was studied. Unlike the linear regression, the classification method apply accuracy, precision, F1 score and related indicators to measure the validty of the model. Splitting the entire dataframe between the train and test sets are essential to avoid biased results. The `train_df` is used only to study and determine the appropriate model.
For this study, the split of dataframe between train and test sets conducted with the ratio of 4:1. It is important to ramdomly allocate the incidences between the data sets. First `set.seed` will ramdomize the incidences in the frame, followed by the division. Once the statistical model is discovered and validated, the same model will be used to 
```{r Train-Test Split, echo=TRUE}
#Train,Test Split
library(caTools)
set.seed(1234)
sample <- sample.split(df, SplitRatio = 0.75) #Randomly set identifier
train_df <- subset(df, sample==TRUE) #Train dataset
test_df  <- subset(df, sample==FALSE) #Test dataset
```

#### 4-1. Fitting the model
Discovering the statistically siginificant variable is extremely difficult just by the help of data visualization. The `train_df` is applied to the logistic regression. As our response variable consists only two categories, the binomial distribution is chosen as the `family` in `glm`.
The summary of the first model indicates the AIC score of 171.97 with the 197 degress of freedom. The coefficient gives the insight of full model of logistic regression. In the sumamry the right most column is the P-value of each coefficient in the model. The more they are statistically sigificant, more codes appear on the right side. 
```{r Fitting Train_df, echo=TRUE}
#Logistic Regression: full fitting with train dataset
df_model <- glm(target~., data=train_df, family=binomial(link="logit"))
print(df_model$coefficents)
summary(df_model)
```
The result above inidcates that the variables, `sex`, `cp`, `trestbps`, `thalach`, `oldpeak` and `ca`, are statistically significant. The model is reduced by fitting only important variables. The summary below is the fitted model of only significant variables. The AIC score of 171.89 with the degrees of freedom of 206. The reduced model has lower AIC score which suggests that it is better fitted than the full model. However, before using the reduced model, the ANOVA test is conducted to prove the significance of difference.

```{r Fitting only significant variables, echo=TRUE}
#create a model with the statistically siginifcant variables only
df_model.part <- glm(target~sex+cp+trestbps+thalach+oldpeak+ca, data=train_df, family=binomial(link="logit"))
summary(df_model.part)
print(df_model.part$aic - df_model$aic) #difference of AIC score
```


```{r ANOVA test for two diffent models, echo=TRUE}
#validate that the reduced model is statistically siginifcant over the full model
anova(df_model.part, df_model, test="Chisq")
```


```{r ROC and AUC partial model, echo=TRUE}
#ROC and AUC for the partial model
library(pROC)
par(pty="s")
roc(train_df$target, df_model.part$fitted.values, plot=TRUE, legacy.axes=TRUE,
    col="Red", print.auc = TRUE, print.auc.x=0.35,
    xlab="False Positive Rate",
    ylab="True Positive Rate")
```

```{r Confusion Matrix, echo=TRUE}
#Fiiting the model to test dataset and Confusion Matrix of the fitted model
df_model_fit <- predict(df_model.part, newdata=test_df, type="response")
df_model_confmat <- ifelse(df_model_fit >0.5, 1, 0)

library(caret)
confusionMatrix(factor(df_model_confmat), factor(test_df$target), positive=as.character(1))
```

